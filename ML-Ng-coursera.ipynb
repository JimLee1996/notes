{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera: [Stanford Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome) Lecture Notes\n",
    "\n",
    "by [*JimLee1996*](mailto:lj96cn@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### 什么是机器学习？\n",
    "\n",
    "Tom Mitchell: \n",
    "\n",
    "*A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.*\n",
    "\n",
    "- E = the experience of playing many games of checkers\n",
    "- T = the task of playing checkers.\n",
    "- P = the probability that the program will win the next game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### 符号约定\n",
    "\n",
    "- $x_j^{(i)}$ 第i个训练样本中的第j个值\n",
    "- $x^{(i)}$ 第i个训练样本，包含所有特征的向量\n",
    "- $m$ 训练样本的总数\n",
    "- $n=|x^{(i)}|$ 特征的数量\n",
    "\n",
    "### Hypothesis Function\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0+\\theta_1 x_1+...+\\theta_n x_n = \\begin{bmatrix}\\theta_0 & \\theta_1 & \\cdots & \\theta_n\\end{bmatrix}\\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}  =\\theta^T x$$\n",
    "\n",
    "$$X = \\begin{bmatrix} {x^{(0)}}^T \\\\ {x^{(1)}}^T \\\\ \\vdots \\\\ {x^{(m)}}^T\\end{bmatrix}$$\n",
    "\n",
    "是以行为单位储存训练样本的，可以当作$m\\times1$的向量,每一行存储的是一组训练样本$x^{(i)}$，维度为$(n+1)\\times1$。\n",
    "\n",
    "因此假设集可以表示为\n",
    "\n",
    "$$h_\\theta(X) = X\\theta$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{(h_\\theta(x^{(i)})-y^{(i)})}^2$$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "while(not convergence):\n",
    "{\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)}$$\n",
    "\n",
    "$$(j = 0, 1, \\cdots, n)$$\n",
    "\n",
    "}\n",
    "\n",
    "向量化计算方法:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)$$\n",
    "\n",
    "$$\\theta:=\\theta-\\frac{\\alpha}{m}X^T(X\\theta-y)$$\n",
    "\n",
    "### Feature Normalization\n",
    "\n",
    "改变输入变量的极差，使他们的范围大致相同，可以加速gradient descent。方法:\n",
    "\n",
    "- feature scaling 用输入变量除以极差\n",
    "- mean normalization 输入变量减去平均值\n",
    "\n",
    "两者结合起来有\n",
    "\n",
    "$$x_i:=\\frac{x_i-\\mu_i}{s_i}$$\n",
    "\n",
    "- $u_i$ 输入变量中所有feature(i)数值的均值\n",
    "- $s_i$ 标准差或极差(max-min)\n",
    "\n",
    "### Gradient Descent Tips\n",
    "\n",
    "- 随着迭代次数增加，$J(\\theta)$不增，如果发生增长，需要减小学习率$\\alpha$\n",
    "- 学习率$\\alpha$足够小时，每次迭代定会下降，不过收敛速度会很慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "$$h_\\theta(x)=g(\\theta^Tx)$$\n",
    "\n",
    "其中:\n",
    "\n",
    "- $g(z)=\\frac{1}{1+e^{-z}}$, sigmoid函数为激活函数\n",
    "\n",
    "- $z=\\theta^T x$\n",
    "\n",
    "$h_\\theta$给出输出结果为1的概率。\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "为判决$y=1$和$y=0$的分界线，通常当$\\theta^Tx\\geq0$判为1\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "价值函数与Linear Regression中的不同，因为Logistic Regression输出是波状的,会有许多极点，因此不应该用凸函数。\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^mCost(h_\\theta(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "$$Cost(h_\\theta(x),y)=-ylog(h_\\theta(x))-(1-y)log(1-h_\\theta(x))$$\n",
    "\n",
    "### 向量化表示\n",
    "\n",
    "$$h=g(X\\theta)$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\left[y^Tlog(h)+(1-y)^Tlog(1-h)\\right]$$\n",
    "\n",
    "$$\\theta:=\\theta-\\frac{\\alpha}{m}X^T(h-y)$$\n",
    "\n",
    "### 高级方法，使用计算优化库\n",
    "\n",
    "使用优化库(e.g. fminunc())，需要给出\n",
    "\n",
    "- $J(\\theta)$\n",
    "- $\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$\n",
    "\n",
    "### Regularization\n",
    "\n",
    "**解决过拟合(overfitting)的问题**\n",
    "\n",
    "- High bias or underfitting 函数过简单或使用了太少特征\n",
    "- High variance or overfitting 函数极好的拟合了训练集，但是不能很好的预测新数据。函数过于复杂，生成了许多无关的曲线或者转角\n",
    "\n",
    "两种解决过拟合的方法:\n",
    "\n",
    "- 1.减少特征数量\n",
    "- 2.Regulation(保留所有特征，但减小$\\theta_j$的值)\n",
    "\n",
    "#### Vectorized Regularized Linear Regression\n",
    "\n",
    "Cost Function,\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)+\\frac{\\lambda}{2m}(\\theta^T\\theta-{\\theta_0}^2)$$\n",
    "\n",
    "Gradient,\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial\\theta}=\\frac{1}{m}X^T(X\\theta-y)+\\frac{\\lambda}{m}\\theta'$$\n",
    "\n",
    "$$\\theta'=\\theta[\\theta_0 = 0]$$\n",
    "\n",
    "#### Vectorized Regularized Logistic Regression\n",
    "\n",
    "$$h=g(X\\theta)$$\n",
    "\n",
    "Cost Function,\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\left[y^Tlog(h)+(1-y)^Tlog(1-h)\\right]+\\frac{\\lambda}{2m}(\\theta^T\\theta-{\\theta_0}^2)$$\n",
    "\n",
    "Gradient,\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial\\theta}=\\frac{1}{m}X^T(h-y)+\\frac{\\lambda}{m}\\theta'$$\n",
    "\n",
    "$$\\theta'=\\theta[\\theta_0 = 0]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "### 诞生想法\n",
    "\n",
    "在复杂的数据集中使用Linear Regression是不明智的。新特征的增长速度: $O(\\frac{n^2}{2})$(考虑所有平方项), $O(n^3)$(考虑所有立方项)\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- dendrites: input features $x_1 \\cdots x_n$\n",
    "\n",
    "- axons: output $h_\\theta(x)$\n",
    "\n",
    "- sigmoid(logistic) activation function: $\\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "- input layer, hiden layers, output layer\n",
    "\n",
    "- $a_i^{(j)}$ \"activation\" of unit $i$ in layer $j$\n",
    "\n",
    "- $\\Theta^{(j)}$ matrix of weights controlling function mapping from layer $j$ to layer $j+1$. 如果layer $j$有$s_j$个units,layer $j+1$有$s_{j+1}$个units, $\\Theta^{(j)}$的维度为$s_{j+1}\\times (s_j+1)$. $+1$来自于bias nodes. 输出不包括bias nodes. $\\Theta$**的横向为输出，纵向为输入**\n",
    "\n",
    "### 模型表示\n",
    "\n",
    "我们引入一个中间量$z$,\n",
    "\n",
    "$$z^{(j)}=\\Theta^{(j-1)}a^{(j-1)}$$\n",
    "\n",
    "$$a^{(j)}=g(z^{(j)})$$\n",
    "\n",
    "然后我们需要增加一个bias node $a_0^{(j)}=1$\n",
    "\n",
    "$$z^{(j+1)}=\\Theta^{(j)}a^{(j)}$$\n",
    "\n",
    "最终有,\n",
    "\n",
    "$$h_\\Theta(x)=a^{(j+1)}=g(z^{(j+1)})$$\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "类似于下列形式\n",
    "\n",
    "$$h_\\Theta(x)= \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "首先定义几个符号\n",
    "\n",
    "- L = 神经网络的层数\n",
    "\n",
    "- $s_l$ = layer $l$包含units的个数(不含bias unit)\n",
    "\n",
    "- $K$ = **分类个数**/输出层units的个数\n",
    "\n",
    "神经网络的Cost Function略显复杂\n",
    "\n",
    "$$J(\\Theta)=- \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2$$\n",
    "\n",
    "Note:\n",
    "\n",
    "- the double sum simply adds up the logistic regression costs calculated for each cell in the output layer\n",
    "- the triple sum simply adds up the squares of all the individual $\\Theta$s in the entire network\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "Goal: $min_\\Theta J(\\Theta)$\n",
    "\n",
    "Need: $\\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta)$\n",
    "\n",
    "反向传导中，我们需要计算每个节点的$\\delta_j^{(l)}$, 即\"error\" of node j in layer l(回忆$a_j^{(l)}$是layer $l$ node $j$的activation).\n",
    "\n",
    "在最后一层(也是output layer)，有\n",
    "$\\delta^{(L)}=a^{(L)}-y$.\n",
    "\n",
    "对于其他层有, $\\delta^{(l)}=\\left((\\Theta^{(l)})^T\\delta^{(l+1)}\\right).*g'(z^{(l)})$. 其中, $g'(z)=g(z).*(1-g(z))$\n",
    "\n",
    "因此上式可以写成\n",
    "\n",
    "$$\\delta^{(l)}=\\left((\\Theta^{(l)})^T\\delta^{(l+1)}\\right).*a^{(l)}.*(1-a^{(l)})$$\n",
    "\n",
    "所以可以计算偏导\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial\\Theta_{i,j}^{(l)}}=\\frac{1}{m}\\sum_{t=1}^ma_j^{(t)(l)}\\delta_i^{(t)(l+1)}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial\\Theta}=\\frac{1}{m}\\sum_{t=1}^m\\delta^{(l+1)}(a^{(l)})^T$$\n",
    "\n",
    "- 注意此处$\\Theta^{l}$的维度是$s_{l+1}\\times s_l$, 并没有包括第0列(即未包含与bias unit有关的列)\n",
    "\n",
    "**算法总结**\n",
    "\n",
    "给出训练集 $\\{(x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})\\}$\n",
    "\n",
    "- 令所有的$\\Delta^{(l)}_{i,j}:=0$\n",
    "\n",
    "对于所有的训练样本\n",
    "\n",
    "- $a^{(1)}:=x^{(t)}$\n",
    "- 计算forward propagation\n",
    "- $\\delta^{(L)}:=a^{(L)}-y^{(t)}$\n",
    "- 使用公式$\\delta^{(l)}=\\left((\\Theta^{(l)})^T\\delta^{(l+1)}\\right).*a^{(l)}.*(1-a^{(l)})$计算$\\delta^{(L-1)}, \\delta^{(L-1)}, \\cdots, \\delta^{(2)}$\n",
    "- $\\Delta^{(l)}:=\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T$\n",
    "- $D_{i,j}^{(l)}:=\\dfrac{1}{m}\\left(\\Delta_(i,j)^{(l)}+\\lambda\\Theta_{i,j}^{(l)}\\right)$ if $j\\neq0$\n",
    "- $D_{i,j}^{(l)}:=\\dfrac{1}{m}\\Delta_(i,j)^{(l)}$ if $j=0$\n",
    "\n",
    "### 算法实现: 参数展开\n",
    "\n",
    "在使用类似`fminunc()`的优化函数时，我们需要将参数展开成vector的形式。在神经网络的价值函数和梯度中，我们需要将参数unrolling。\n",
    "\n",
    "### Gradient Checking\n",
    "\n",
    "用于检查反向传导正常工作，梯度估算的公式为\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\frac{J(\\Theta_1, \\cdots, \\Theta_j + \\epsilon, \\cdots, \\Theta_n) - J(\\Theta_1, \\cdots, \\Theta_j - \\epsilon, \\cdots, \\Theta_n)}{2\\epsilon}$$\n",
    "\n",
    "通常取${\\epsilon = 10^{-4}}$\n",
    "\n",
    "### Random Initialization\n",
    "\n",
    "使用随机初始化参数，可以有效避免神经网络对称传导(不利于机器学习)\n",
    "\n",
    "$\\Theta^{(l)}_{ij}$在$[-\\epsilon,\\epsilon]$范围中，其中$\\epsilon = \\frac{\\sqrt{6}}{\\sqrt{\\mathrm{Loutput} + \\mathrm{Linput}}}$\n",
    "\n",
    "因此，可以用下列公式表示\n",
    "\n",
    "$$\\Theta^{(l)} =  2 \\epsilon \\; \\mathrm{rand}(\\mathrm{Loutput}, \\mathrm{Linput} + 1)    - \\epsilon$$\n",
    "\n",
    "\n",
    "### 总结\n",
    "第一步，选择一个网络结构，每一层含有多少个hidden units以及总共包含多少个hidden layer\n",
    "\n",
    "- input units数量 = $X^{(i)}$特征的维度\n",
    "- output units数量 = classes的数量\n",
    "- 每层hidden units的数量，通常越多越好，但需要权衡computation\n",
    "- 默认1个hidden layer。多余1个则尽量保证每个hidden layer中hidden units数量相同\n",
    "\n",
    "**训练神经网络**\n",
    "\n",
    "1. Randomly initialize the weights\n",
    "2. Implement forward propagation to get $h_\\theta(x^{(i)})$\n",
    "3. Implement the cost function\n",
    "4. Implement backpropagation to compute partial derivatives\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for Applying Machine Learning and Deciding What to Try Next\n",
    "\n",
    "Errors in your predictions can be troubleshooted by:\n",
    "\n",
    "- Getting more training examples\n",
    "- Trying smaller sets of features\n",
    "- Trying additional features\n",
    "- Trying polynomial features\n",
    "- Increasing or decreasing $\\lambda$\n",
    "\n",
    "### Evaluating a Hypothesis\n",
    "\n",
    "With a given dataset of training examples, we can split up the data into two sets: a **training set** and a **test set**.\n",
    "\n",
    "- Learn $\\Theta$ and minimize $J_{train}(\\Theta)$ using the training set\n",
    "- Compute the test set error $J_{test}(\\Theta)$\n",
    "\n",
    "### Test Set Error\n",
    "\n",
    "- For linear regression:\n",
    "\n",
    "$$J_{test}(\\Theta) = \\dfrac{1}{2m_{test}} \\sum_{i=1}^{m_{test}}(h_\\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$$\n",
    "\n",
    "- For classification\n",
    "\n",
    "Misclassification error (aka 0/1 misclassification error):\n",
    "\n",
    "$err(h_\\Theta(x),y) = 1$, if $h_\\Theta(x) \\geq 0.5$ and $y = 0$ or $h_\\Theta(x) < 0.5$ and $y = 1$\n",
    "\n",
    "$err(h_\\Theta(x),y) = 0$, otherwise\n",
    "\n",
    "$\\text{Test Error} = \\dfrac{1}{m_{test}} \\sum^{m_{test}}_{i=1} err(h_\\Theta(x^{(i)}_{test}), y^{(i)}_{test})$\n",
    "\n",
    "### Model Selection and Train/Validation/Test Sets\n",
    "\n",
    "通常这样划分我们的数据集:\n",
    "\n",
    "- Training Set: 60%\n",
    "- Cross Validation Set: 20%\n",
    "- Test Set: 20%\n",
    "\n",
    "通过下面步骤finetune hyperparams\n",
    "\n",
    "1. 设定不同的多项式阶数$d$，用Training Set训练$\\Theta$<br>\n",
    "2. 用Cross Validtion Set找到使得误差最小的多项式阶数$d$<br>\n",
    "3. 使用Test Set评估我们的$\\Theta^{(d)}$\n",
    "\n",
    "### Diagnosing Bias vs. Variance\n",
    "\n",
    "High bias is underfitting and high variance is overfitting. We need to find a golden mean between these two.\n",
    "\n",
    "The training error will tend to decrease as we increase the degree d of the polynomial.\n",
    "\n",
    "At the same time, the cross validation error will tend to **decrease** as we increase d up to a point, and then it will **increase** as d is increased, forming a convex curve.\n",
    "\n",
    "- High bias (underfitting): both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ will be high. Also, $J_{CV}(\\Theta)$≈$J_{train}(\\Theta)$\n",
    "\n",
    "- High variance (overfitting): $J_{train}(\\Theta)$ will be low and $J_{CV}$ will be much greater than $J_{train}(\\Theta)$\n",
    "\n",
    "### Regularization and Bias/Variance\n",
    "\n",
    "- Large $\\lambda$: High bias (underfitting)<br>\n",
    "both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ will be high (underfitting /high bias)\n",
    "\n",
    "- Intermediate $\\lambda$: just right <br>\n",
    "$J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ are somewhat low and $J_{train}(\\Theta)$≈$J_{CV}(\\Theta)$.\n",
    "\n",
    "- Small $\\lambda$: High variance (overfitting) <br>\n",
    "$J_{train}(\\Theta)$ is low and $J_{CV}(\\Theta)$ is high (high variance/overfitting).\n",
    "\n",
    "In order to choose the model and the regularization $\\lambda$, we need:\n",
    "\n",
    "1. Create a list of $\\lambda$s (i.e. $\\lambda$∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});\n",
    "2. Create a set of models with different degrees or any other variants.\n",
    "3. Iterate through the $\\lambda$s and for each $\\lambda$ go through all the models to learn some $\\Theta$.\n",
    "4. Compute the cross validation error using the learned $\\Theta$ (computed with $\\lambda$) on the JCV($\\Theta$) without regularization or $\\lambda$ = 0.\n",
    "5. Select the best combo that produces the lowest error on the cross validation set.\n",
    "6. Using the best combo $\\Theta$ and $\\lambda$, apply it on Jtest($\\Theta$) to see if it has a good generalization of the problem.\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "样本量m很小时，很容易得到小误差，随着m增大，误差值逐渐增大，直到某个特定m后趋于平稳\n",
    "\n",
    "- With high bias:\n",
    "    - Low training set size: $J_{train}(\\Theta)$很小而$J_{CV}(\\Theta)$很大\n",
    "    - Large training set size: $J_{train}(\\Theta)$和$J_{CV}(\\Theta)$接近且都很大\n",
    "\n",
    "- With high variance:\n",
    "    - Low training set size: $J_{train}(\\Theta)$很小而$J_{CV}(\\Theta)$很大\n",
    "    - Large training set size: $J_{train}(\\Theta)$增加而$J_{CV}(\\Theta)$持续减小，$J_{train}(\\Theta) < J_{CV}(\\Theta)$并且还差相当一部分\n",
    "\n",
    "\n",
    "### 下一步怎么做?\n",
    "\n",
    "- Getting more training examples: Fixes high variance\n",
    "- Trying smaller sets of features: Fixes high variance\n",
    "- Adding features: Fixes high bias\n",
    "- Adding polynomial features: Fixes high bias\n",
    "- Decreasing $\\lambda$: Fixes high bias\n",
    "- Increasing $\\lambda$: Fixes high variance\n",
    "\n",
    "### 诊断神经网络\n",
    "\n",
    "- 少参数神经网络倾向于underfitting，但计算简单\n",
    "- 多参数神经网络倾向于overfitting，可使用regularization(增大$\\lambda$)，但计算复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning System Design\n",
    "\n",
    "### 工作的优先级\n",
    "\n",
    "很难确定哪一项可能有帮助！\n",
    "\n",
    "- 更多数据！\n",
    "- 使用更复杂精致的features\n",
    "- 用算法预处理数据\n",
    "\n",
    "### 误差分析\n",
    "\n",
    "解决机器学习的推荐方法\n",
    "\n",
    "1. 从一个简单的算法开始，快速实现，早期测试\n",
    "2. 绘制学习曲线，看是更多数据，还是更多特征等有所帮助\n",
    "3. 误差分析，分析cross validation set中的误差，画出趋势\n",
    "\n",
    "### Error Metrics for Skewed Classes\n",
    "\n",
    "有时候，不能说误差降低了就是算法提升了\n",
    "\n",
    ">例如: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm.\n",
    "\n",
    "这也被称为Skewed Classes，也就是这个class在我们整个数据集中所占比重很小。\n",
    "\n",
    "此时我们用另一种评判标准**Precision/Recall**\n",
    "\n",
    "\n",
    "| Name | Actual | Predict |\n",
    "|------|--------|---------|\n",
    "|  True Positive| 1 | 1 |\n",
    "| True negative | 0 | 0 |\n",
    "| False Positive| 0 | 1 |\n",
    "| False Positive| 1 | 0 |\n",
    "\n",
    "- Precision: of all patients we predicted where y=1, what fraction actually has cancer?(我们探测到的阳性中，有几个是真的?)\n",
    "\n",
    "$$\\dfrac{\\text{True Positives}}{\\text{Total number of predicted positives}}\n",
    "= \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False positives}}$$\n",
    "\n",
    "- Recall: Of all the patients that actually have cancer, what fraction did we correctly detect as having cancer?(在所有阳性样本中，我们又能探测到几个?)\n",
    "\n",
    "$$\\dfrac{\\text{True Positives}}{\\text{Total number of actual positives}}= \\dfrac{\\text{True Positives}}{\\text{True Positives}+\\text{False negatives}}$$\n",
    "\n",
    "### Trading Off Precision and Recall\n",
    "\n",
    "We might want a **confident** prediction of two classes using logistic regression. One way is to increase our threshold:\n",
    "\n",
    "- Predict 1 if: $h_{\\theta}(x) \\geq 0.7$\n",
    "- Predict 0 if: $h_{\\theta}(x) < 0.7$\n",
    "\n",
    "Doing this, we will have **higher precision** but **lower recall** (refer to the definitions in the previous section).\n",
    "\n",
    "In the opposite example, we can lower our threshold:\n",
    "\n",
    "- Predict 1 if: $h_{\\theta}(x) \\geq 0.3$\n",
    "- Predict 0 if: $h_{\\theta}(x) < 0.3$\n",
    "\n",
    "That way, we get a very **safe** prediction. This will cause **higher recall** but **lower precision**.\n",
    "\n",
    "通常情况下, 我们使用**F Score**来评判(其值越大越好，通常在cross validation set中进行), 即\n",
    "\n",
    "$$F_{Score}=2\\frac{PR}{P+R}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "### Optimization Objective\n",
    "\n",
    "回忆logistic regression中的知识。在SVM中，需要优化cost function。\n",
    "\n",
    "\n",
    "上图分别对应 $cost_1(z)$ 和 $cost_0(z)$ (注意 $cost_1(z)$ 是 y=1 时的cost, 而 $cost_0(z)$ 是 y=0 时的cost), 我们可以用下列公式定义它们 (其中 k 为任意常数，定义了斜率):\n",
    "\n",
    "$z = \\theta^Tx$\n",
    "\n",
    "$\\text{cost}_1(z) = \\max(0, k(1-z))$\n",
    "\n",
    "$\\text{cost}_0(z) = \\max(0, k(1+z))$\n",
    "\n",
    "于是cost function可写成（为了更简单计算，扔掉了m，并令$C=\\frac{1}{\\lambda}$），\n",
    "\n",
    "$$J(\\theta) = C\\sum_{i=1}^m y^{(i)} \\ \\text{cost}_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) \\ \\text{cost}_0(\\theta^Tx^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "想要减少overfitting，减小C。反之，为了增强regularization，增大C。而且，我们更加急切的想要$\\Theta^Tx \\geq 1$ if y=1 and $\\Theta^Tx \\leq -1$ if y=0.\n",
    "\n",
    "### Large Margin Intuition\n",
    "\n",
    "回忆logistic regression的decision boundary（一条分割正负样本的线）。在SVM中，这条线有一个特殊的性质，就是尽可能的远离both the positive and the negative examples.\n",
    "\n",
    "The distance of the decision boundary to the nearest example is called the **margin**. Since SVMs maximize this margin, it is often called a Large Margin Classifier.\n",
    "\n",
    "The SVM will separate the negative and positive examples by a **large margin**.\n",
    "\n",
    "This large margin is only achieved when **C is very large**.\n",
    "\n",
    "If we have **outlier examples** that we don't want to affect the decision boundary, then we can **reduce C**.\n",
    "\n",
    "### Kernels\n",
    "\n",
    "**Kernels** allow us to make complex, non-linear classifiers using Support Vector Machines.\n",
    "\n",
    "Given x, compute new feature depending on proximity to landmarks $l^{(1)}$, $l^{(2)}$, $l^{(3)}$.\n",
    "\n",
    "To do this, we find the \"similarity\" of x and some landmark $l^{(i)}$:\n",
    "\n",
    "$$f_i = similarity(x, l^{(i)}) = \\exp(-\\dfrac{||x - l^{(i)}||^2}{2\\sigma^2})$$\n",
    "\n",
    "This \"similarity\" function is called a **Gaussian Kernel**. It is a specific example of a kernel.\n",
    "\n",
    "The similarity function can also be written as follows:\n",
    "\n",
    "$$f_i = similarity(x, l^{(i)}) = \\exp(-\\dfrac{\\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\\sigma^2})$$\n",
    "\n",
    "If x and the landmark are close, then the similarity will be close to 1, and if x and the landmark are far away from each other, the similarity will be close to 0.\n",
    "\n",
    "Each landmark gives us the features in our hypothesis:\n",
    "\n",
    "$$\\begin{matrix}l^{(1)} \\rightarrow f_1 \\\\ l^{(2)} \\rightarrow f_2 \\\\ l^{(3)} \\rightarrow f_3 \\\\ \\cdots \\\\ h_\\Theta(x) = \\Theta_1f_1 + \\Theta_2f_2 + \\Theta_3f_3 + \\cdots \\end{matrix}$$\n",
    "\n",
    "一个获得landmarks的方法就是选择所有训练样本所在的位置. 每一个landmark都有一个训练样本对应.\n",
    "\n",
    "我们对样本集使用kernel的映射生成feature vector。（别忘了为$\\theta_0$加上$f_0$）\n",
    "\n",
    "$$x^{(i)} \\rightarrow \\begin{bmatrix}f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \\\\ f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \\\\ \\vdots \\\\ f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \\end{bmatrix}$$\n",
    "\n",
    "可以开始让机器学习了！\n",
    "\n",
    "$$\\min_{\\Theta} C \\sum_{i=1}^m y^{(i)}\\text{cost}_1(\\Theta^Tf^{(i)}) + (1 - y^{(i)})\\text{cost}_0(\\theta^Tf^{(i)}) + \\dfrac{1}{2}\\sum_{j=1}^n \\Theta^2_j$$\n",
    "\n",
    "**Notes!**\n",
    "- 可以使用'liblinear'或者'libsvm'实现SVMs。需要自己选择参数C，核函数（不选择默认做linear regression）\n",
    "- 使用Gaussian Kernel前，一定要做feature scaling\n",
    "- not all similarity functions are valid kernels. They must satisfy \"Mercer's Theorem\" which guarantees that the SVM package's optimizations run correctly and do not diverge.\n",
    "\n",
    "### Multi-class Classification\n",
    "\n",
    "可以使用one-vs-all方法\n",
    "\n",
    "### Logistic Regression vs. SVMs\n",
    "\n",
    "- If n is large (relative to m), then use logistic regression, or SVM without a kernel (the \"linear kernel\")\n",
    "- If n is small and m is intermediate, then use SVM with a Gaussian Kernel\n",
    "- If n is small and m is large, then manually create/add more features,  then use logistic regression or SVM without a kernel.\n",
    "\n",
    "In the first case, we don't have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.\n",
    "\n",
    "**Note**: a neural network is likely to work well for any of these situations, but may be slower to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Unsupervised Learning介绍\n",
    "\n",
    "使用unlabeled的训练集，可用于\n",
    "\n",
    "- market segmentation\n",
    "- social network analysis\n",
    "- organizing computer cluster\n",
    "- astronomical data analysi\n",
    "\n",
    "### K-Means Algorithm\n",
    "\n",
    "最常用的自动聚合相关数据子集的算法\n",
    "\n",
    "1. 随即在数据集中初始化几个点(cluster centroids)\n",
    "2. 根据数据集中的点与cluster centroids的距离远近分组\n",
    "3. 移动cluster centroids，移动到聚类点的质心\n",
    "4. 重复运行2和3直到找到聚类\n",
    "\n",
    "算法描述\n",
    "```\n",
    "Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)\n",
    "Repeat:\n",
    "    for i = 1 to m:\n",
    "        c(i) := index (from 1 to K) of cluster centroid closet to x(i)\n",
    "    for k = 1 to K:\n",
    "        mu(k) := average(mean) of points assigned to cluster K\n",
    "```\n",
    "\n",
    "其中$c^{(i)} = argmin_k\\ ||x^{(i)} - \\mu_k||^2$. That is, each $^{c(i)}$ contains the index of the centroid that has minimal distance to $x^{(i)}$.\n",
    "\n",
    "### Optimization Objective\n",
    "\n",
    "- cost function (**distortion**)\n",
    "\n",
    "$$J(c^{(i)},\\cdots,c^{(m)},\\mu_1,\\cdots,\\mu_K) = \\frac{1}{m}\\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2$$\n",
    "\n",
    "- $c^{(i)}$ = index of cluster (1,2,...,K) to which example x(i) is currently assigned\n",
    "- $μ_k$= cluster centroid k (μk∈ℝn)\n",
    "- $μ_c^{(i)}$ = cluster centroid of cluster to which example x(i) has been assigned\n",
    "\n",
    "### Random Initialization\n",
    "\n",
    "K-means **can get stuck in local optima**. To decrease the chance of this happening, you can run the algorithm on many different random initializations. In cases where K<10 it is strongly recommended to run a loop of random initializations.\n",
    "\n",
    "### Choosing the Number of Clusters\n",
    "\n",
    "选K值的方法相当随意和模糊.\n",
    "\n",
    "- **The elbow method**: 绘制cost J和clusters K的图像. 随着K增大, J持续减小, 然后趋向平稳. 选择曲线开始趋向平稳处的K值.\n",
    "\n",
    "    然后很多情况下,上述图像都很平缓,肉眼看不出K值\n",
    "\n",
    "    **Note**: J总会**下降**随着K上升. 唯一的例外就是卡在local optimum了.\n",
    "\n",
    "- Another way to choose K is to observe how well k-means performs on a downstream purpose. In other words, you choose K that proves to be most useful for some goal you're trying to achieve from using these clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "- Motivation I: Data Compression\n",
    "\n",
    "    - 如果有许多冗余数据,我们想降低features的维度.\n",
    "    - 我们可以找到两个高度相关的features, 绘出图像，然后用一条新的曲线表述它们.\n",
    "\n",
    "    Dimensionality reduction 能够减少数据存储占用的空间, 并加速算法的计算速度.\n",
    "\n",
    "- Motivation II: Visualization\n",
    "    \n",
    "    提取新特征$z_1$, $z_2$(甚至$z_3$)帮助我们有效的概括所有其他特征，方便我们实现数据可视化.\n",
    "\n",
    "### Principal Component Analysis (PCA) Problem Formulation\n",
    "\n",
    "给出两个features$x_1$和$x_2$, 我们想找到一条能够同时有效描述这两个特征的曲线. 我们把旧的features映射到这条曲线上得到新的feature.\n",
    "\n",
    "PCA的目标就是尽可能减少the average of all the distances of every feature to the projection line. 这叫做**projection error**.\n",
    "\n",
    "注意！**PCA不是linear regression**\n",
    "\n",
    "### PCA算法\n",
    "\n",
    "#### Preprocessing Data\n",
    "\n",
    "- feature scaling/mean normalization\n",
    "\n",
    "$$x_j^{(i)} = \\dfrac{x_j^{(i)} - \\mu_j}{s_j}$$\n",
    "\n",
    "1. Compute \"covariance matrix\"\n",
    "\n",
    "$$\\Sigma_{n \\times n} = \\frac{1}{m}X^TX$$\n",
    "\n",
    "2.  Compute \"eigenvectors\" of covariance matrix $\\Sigma$\n",
    "\n",
    "[U, S, V] = svd($\\Sigma$);\n",
    "\n",
    "3. Take the first k columns of the U matrix and compute z\n",
    "\n",
    "$$Ureduce_{n \\times k} =\\text{U(:, 1:k)}$$\n",
    "$$Z = X \\cdot Ureduce_{n \\times k}$$\n",
    "\n",
    "### Reconstruction from Compressed Representation\n",
    "上述过程的逆过程，两边同乘逆矩阵\n",
    "\n",
    "### Choosing the Number of Principal Components\n",
    "- Given the average squared projection error: $\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$\n",
    "- Also given the total variation in the data: $\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)}||^2$\n",
    "- Choose k to be the smallest value such that: $\\dfrac{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\\dfrac{1}{m}\\sum^m_{i=1}||x^{(i)}||^2} \\leq 0.01$\n",
    "\n",
    "In other words, the squared projection error divided by the total variation should be less than one percent, so that **99% of the variance is retained**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "定义了一个\"模型\"p(x), 告知不是anomaly的概率. 同时使用了阈值$\\epsilon$来判决哪些时正常的,哪些是异常的. 如果模型判断了过多异常,考虑降低阈值￥$\\epsilon$.\n",
    "\n",
    "### Gaussian Distribution\n",
    "\n",
    "$$p(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{(2\\pi)}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}$$\n",
    "\n",
    "其中$\\mu = \\frac{1}{m}\\sum_{i=1}^m x^{(i)}$, $\\sigma^2 = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)} - \\mu)^2$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Choose features $x_i$ that you think might be indicative of anomalous examples.\n",
    "\n",
    "2. Fit parameters$\\mu_1,\\cdots,\\mu_n,\\sigma_1^2,\\cdots,\\sigma_n^2$\n",
    "\n",
    "3. Calculate $\\mu_j = \\dfrac{1}{m}\\displaystyle \\sum_{i=1}^m x_j^{(i)}$\n",
    "\n",
    "4. Calculate $\\sigma^2_j = \\dfrac{1}{m}\\displaystyle \\sum_{i=1}^m(x_j^{(i)} - \\mu_j)^2$\n",
    "\n",
    "5. Given a new example x, compute p(x):\n",
    "\n",
    "$$p(x) = \\prod^n_{j=1} p(x_j;\\mu_j,\\sigma_j^2))$$\n",
    "\n",
    "Anomaly if $p(x)<\\epsilon$.\n",
    "\n",
    "### Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "When do we use anomaly detection and when do we use supervised learning?\n",
    "\n",
    "Use anomaly detection when...\n",
    "\n",
    "- We have a very small number of positive examples (y=1 ... 0-20 examples is common) and a large number of negative (y=0) examples.\n",
    "\n",
    "- We have many different \"types\" of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.\n",
    "\n",
    "Use supervised learning when...\n",
    "\n",
    "- We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes.\n",
    "\n",
    "- We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.\n",
    "\n",
    "### Choosing What Features to Use\n",
    "\n",
    "The features will greatly affect how well your anomaly detection algorithm works.\n",
    "\n",
    "We can check that our features are **gaussian** by plotting a histogram of our data and checking for the bell-shaped curve.\n",
    "\n",
    "Some transforms we can try on an example feature x that does not have the bell-shaped curve are:\n",
    "\n",
    "- log(x)\n",
    "- log(x+1)\n",
    "- log(x+c)\n",
    "- $\\sqrt x$\n",
    "- $x^{\\frac{1}{3}}$\n",
    "\n",
    "There is an **error analysis procedure** for anomaly detection that is very similar to the one in supervised learning.\n",
    "\n",
    "### Anomaly Detection using the Multivariate Gaussian Distribution\n",
    "\n",
    "The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.\n",
    "\n",
    "$$p(x;\\mu,\\Sigma) = \\dfrac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} exp(-1/2(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$$\n",
    "\n",
    "The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours.\n",
    "\n",
    "Varying $\\Sigma$ changes the shape, width, and orientation of the contours. Changing $\\mu$ will move the center of the distribution.\n",
    "\n",
    "The multivariate Gaussian model can automatically capture correlations between different features of x.\n",
    "\n",
    "However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for $\\Sigma$ to be invertible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "### 符号约定\n",
    "\n",
    "- $n_u$ = number of users\n",
    "- $n_m$ = number of movies\n",
    "- $r(i,j)$ = 1, if user j has rated movie i\n",
    "- $y(i,j)$ = rating given by user j to movie i\n",
    "\n",
    "- $θ^{(j)}$ = parameter vector for user j\n",
    "- $x^{(i)}$ = feature vector for movie i\n",
    "\n",
    "    For user j, movie i, predicted rating: $(\\theta^{(j)})^T(x^{(i)})$\n",
    "\n",
    "- $m^{(j)}$ = number of movies rated by user j\n",
    "\n",
    "To get parameters $\\theta$ for all users, we do the following\n",
    "\n",
    "$$min_{\\theta^{(1)},\\cdots,\\theta^{(n_u)}} = \\dfrac{1}{2}\\displaystyle \\sum_{j=1}^{n_u}  \\sum_{i:r(i,j)=1} ((\\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \\dfrac{\\lambda}{2} \\sum_{j=1}^{n_u} \\sum_{k=1}^n(\\theta_k^{(j)})^2$$\n",
    "\n",
    "### Collaborative Filtering Algorithm\n",
    "\n",
    "同时迭代features和parameters有助于加速运算:\n",
    "\n",
    "$$J(x,\\theta) = \\dfrac{1}{2} \\displaystyle \\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \\dfrac{\\lambda}{2}\\sum_{i=1}^{n_m} \\sum_{k=1}^{n} (x_k^{(i)})^2 + \\dfrac{\\lambda}{2}\\sum_{j=1}^{n_u} \\sum_{k=1}^{n} (\\theta_k^{(j)})^2$$\n",
    "\n",
    "1. 用小的随机值初始化$x$, $\\theta$, break symmetry\n",
    "\n",
    "2. minimize J\n",
    "\n",
    "3. $\\theta^Tx$预测评分\n",
    "\n",
    "附上vectorized源码\n",
    "\n",
    "```\n",
    "J = 1/2 * sum(sum( ...\n",
    "    (X * Theta' - Y).^2 ...\n",
    "    .*R)) + ...\n",
    "    lambda / 2 *(...\n",
    "    sum(sum(Theta.^2))+sum(sum(X.^2)));\n",
    "\n",
    "\n",
    "% 根据维度凑出来的代码，需要仔细研究\n",
    "X_grad = (X * Theta' - Y).*R * Theta ...\n",
    "    + lambda*X;\n",
    "Theta_grad = ((X * Theta' - Y).*R)' * X ...\n",
    "    + lambda*Theta;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Large Datasets\n",
    "\n",
    "数据集通常达到 m = 100,000,000 的规模. 计算起来会非常吃力.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "$$cost(\\theta,(x^{(i)}, y^{(i)})) = \\dfrac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "$$J_{train}(\\theta) = \\dfrac{1}{m} \\displaystyle \\sum_{i=1}^m cost(\\theta, (x^{(i)}, y^{(i)}))$$\n",
    "\n",
    "算法如下\n",
    "\n",
    "1. 随机打乱数据集\n",
    "\n",
    "2. for $i = 1 \\cdots m$\n",
    "\n",
    "    $\\Theta_j := \\Theta_j - \\alpha (h_{\\Theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_j$\n",
    "\n",
    "This algorithm will only try to fit one training example at a time. This way we can make progress in gradient descent without having to scan all m training examples first. Stochastic gradient descent will be unlikely to converge at the global minimum and will instead wander around it randomly, but usually yields a result that is close enough. Stochastic gradient descent will usually take 1-10 passes through your data set to get near the global minimum.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent can sometimes be even faster than stochastic gradient descent. Instead of using all m examples as in batch gradient descent, and instead of using only 1 example as in stochastic gradient descent, we will use some in-between number of examples b.\n",
    "\n",
    "Typical values for b range from 2-100 or so.\n",
    "\n",
    "For example, with b=10 and m=1000:\n",
    "\n",
    "Repeat:\n",
    "\n",
    "for $i = 1,11,21,31,\\dots,991$:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\dfrac{1}{10} \\displaystyle \\sum_{k=i}^{i+9} (h_\\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$\n",
    "\n",
    "### Stochastic Gradient Descent Convergence\n",
    "\n",
    "How do we choose the learning rate α for stochastic gradient descent? Also, how do we debug stochastic gradient descent to make sure it is getting as close as possible to the global optimum?\n",
    "\n",
    "One strategy is to plot the average cost of the hypothesis applied to every 1000 or so training examples. We can compute and save these costs during the gradient descent iterations.\n",
    "\n",
    "With a smaller learning rate, it is possible that you may get a slightly better solution with stochastic gradient descent. That is because stochastic gradient descent will oscillate and jump around the global minimum, and it will make smaller random jumps with a smaller learning rate.\n",
    "\n",
    "If you increase the number of examples you average over to plot the performance of your algorithm, the plot's line will become smoother.\n",
    "\n",
    "With a very small number of examples for the average, the line will be too noisy and it will be difficult to find the trend.\n",
    "\n",
    "One strategy for trying to actually converge at the global minimum is to slowly decrease α over time. For example $\\alpha = \\dfrac{const1}{iterationNumber + const2}$. However, this is not often done because people don't want to have to fiddle with even more parameters.\n",
    "\n",
    "### Online Learning\n",
    "\n",
    "With a continuous stream of users to a website, we can run an endless loop that gets (x,y), where we collect some user actions for the features in x to predict some behavior y.\n",
    "\n",
    "You can update $\\theta$ for each individual (x,y) pair as you collect them. This way, you can adapt to new pools of users, since you are continuously updating theta.\n",
    "\n",
    "### Map Reduce and Data Parallelism\n",
    "\n",
    "We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
